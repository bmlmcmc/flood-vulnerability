from __future__ import absolute_import
import scrapy
import re
from detikcom.items import DetikcomItem


class DetikSpider(scrapy.Spider):
    name = 'detikcom'
    allowed_domain = 'www.detik.com'
    start_urls = ['https://www.detik.com/search/searchall?query=banjir&page=1&sortby=time&sorttime=0&siteid=2&fromdatex=01%2F01%2F2016&todatex=01%2F01%2F2021&hitperpages=9&siteid=3']
    # https://www.detik.com/search/searchall?query=banjir%20jakarta&siteid=2&sortby=time&fromdatex=01/01/2016&page=2

    def start_requests(self):
        for url in self.start_urls:
            yield scrapy.http.Request(url, callback=self.parse)

    def get_content(self, response):
        item = response.meta['item']
        text = ' '.join([e.strip() for e in response.xpath(
            "//div[@class='detail__body-text itp_bodycontent']/descendant::text()[not(ancestor::a)][not(ancestor::table)]").extract() if not any(ee in e for ee in ['{', '}', 'Baca juga:'])])
        text = re.sub("\s+", " ", text)
        text = re.sub("\s+", " ", text)
        item['content'] = text
        item['tags'] = response.xpath(
            "//a[@class='nav__item']/text()").getall()
        return item

    def parse(self, response):
        header = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36'}

        item = DetikcomItem()
        articles = response.css('div.list.media_rows.list-berita article')
        print("Response: ", response.status)
        for article in articles:

            item['link'] = article.css('a::attr(href)').get()
            item['date'] = article.css('span.date::text').get()
            item['title'] = article.css('h2.title::text').get()
            #item['category'] = article.css('span.category::text').get()
            #get_contents = scrapy.http.Request(article.css('a::attr(href)').get()+'?single=1', callback = self.get_content)
            #get_contents.meta['item'] = item
            # yield get_contents
            yield item

        #next_page = response.urljoin(response.url+'&siteid=2&sortby=time&page=' + str(self.idx))
        #self.idx += 1

        next_page = response.xpath(
            '//div[@class="paging text_center"]//a[@class="selected"]/following-sibling::a/@href')[0].get()
        #print('next_page: ', next_page)

        # print(next_page)
        # yield response.follow(next_page, callback=self.parse, headers = header)
        yield scrapy.http.Request(next_page, callback=self.parse, headers=header)
